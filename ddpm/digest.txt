Directory structure:
└── ddpm/
    ├── data.py
    ├── diffusion.py
    ├── model.py
    ├── train.py
    └── unet_components.py

================================================
File: data.py
================================================
from torchvision import transforms
from torchvision.datasets import CIFAR10, MNIST, CelebA, Flowers102
from torch.utils.data import DataLoader

DATASET_DIR = "/home/alazar/desktop/datasets"
DATALOADER_REGISTRY = {}

def register_dataloader(name):
    """
    A decorator to register a dataloader in the DATALOADERS dictionary.
    """
    def decorator(dataloader_class):
        DATALOADER_REGISTRY[name] = dataloader_class
        return dataloader_class
    return decorator

### CIFAR10 and MNIST datasets






### Register the dataloaders
@register_dataloader("cifar10")
def make_cifar10_loader(batch_size=128, resize=(32, 32)):
    cifar10_dataset = CIFAR10(
        root=DATASET_DIR,
        train=True,
        download=True,
        transform=transforms.Compose(
            [transforms.Resize(resize)] if resize != (32, 32) else []
            + [
                transforms.RandomHorizontalFlip(),
                transforms.ToTensor(),
                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
            ]
        )
    )
    return DataLoader(
        cifar10_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )


@register_dataloader("mnist")
def make_mnist_loader(batch_size=128, resize=(32, 32)):
    mnist_dataset = MNIST(
        root=DATASET_DIR,
        train=True,
        download=True,
        transform=transforms.Compose([
            transforms.Resize(resize),
            transforms.ToTensor(),
            transforms.Normalize((0.5,), (0.5,)),
        ])
    )
    return DataLoader(
        mnist_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )

@register_dataloader("celeba")
def make_celeba_loader(batch_size=128, resize=(32, 32)):
    celeba_dataset = CelebA(
        root=DATASET_DIR,
        split='train',
        download=True,
        transform=transforms.Compose([
            transforms.CenterCrop(178),  # CelebA images are 178x218
            transforms.Resize(resize),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ])
    )
    return DataLoader(
        celeba_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )

@register_dataloader("flowers")
def make_flowers_loader(batch_size=128, resize=(32, 32)):
    flowers_dataset = Flowers102(
        root=DATASET_DIR,
        split='train',
        download=True,
        transform=transforms.Compose([
            transforms.Resize(resize),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ])
    )
    return DataLoader(
        flowers_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )


================================================
File: diffusion.py
================================================
import torch
from tqdm import tqdm
import wandb
from typing import Dict, Tuple
import copy

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Schedule for betas
def cosine_beta_schedule(timesteps, s=0.008)-> torch.Tensor:
    steps = torch.arange(timesteps + 1, dtype=torch.float32)
    alphas_cumprod = torch.cos(((steps / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2
    alphas_cumprod /= alphas_cumprod[0].clone()
    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
    return torch.clamp(betas, 1e-5, 0.999)

def make_diffusion_schedule(step_count, device) -> Dict[str, torch.Tensor]:
    betas = cosine_beta_schedule(step_count)
    alphas = 1 - betas
    alphas_cumprod = torch.cumprod(alphas, dim=0)

    return {
        'betas': betas.to(device),
        'alphas': alphas.to(device),
        'alphas_cumprod': alphas_cumprod.to(device),
        'sqrt_acum': alphas_cumprod.sqrt().to(device),
        'sqrt_1macum': (1 - alphas_cumprod).sqrt().to(device)
    }


def train(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, optimizer: torch.optim.Optimizer, criterion: torch.nn.Module, scheduler: torch.optim.lr_scheduler._LRScheduler, num_epochs: int, step_count: int, schedule: Dict[str, torch.Tensor]) -> torch.nn.Module:
    global_step = 0

    # Extract schedule values more safely
    betas = schedule['betas']
    alphas = schedule['alphas']
    alphas_cumprod = schedule['alphas_cumprod']
    sqrt_acum = schedule['sqrt_acum']
    sqrt_1macum = schedule['sqrt_1macum']

    ema_model = copy.deepcopy(model).eval().requires_grad_(False)

    for epoch in range(num_epochs):
        running_loss = 0.0

        for i, (images, _) in enumerate(tqdm(dataloader)):
            model.train()
            images = images.to(device)

            # Sample random time steps
            t = torch.randint(0, step_count, (images.size(0),), device=device)
            noise = torch.randn_like(images)

            # Add noise to images according to the diffusion schedule
            noisy_images = sqrt_acum[t].reshape(-1, 1, 1, 1) * images + sqrt_1macum[t].reshape(-1, 1, 1, 1) * noise
            predicted_noise = model(noisy_images, t.unsqueeze(1) / step_count)

            loss = criterion(predicted_noise, noise)
            optimizer.zero_grad()
            loss.backward()

            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            # Update EMA model
            decay = 0.9999
            with torch.no_grad():
                for param, ema_param in zip(model.parameters(), ema_model.parameters()):
                    ema_param.data = decay * ema_param.data + (1 - decay) * param.data

            running_loss += loss.item()

            wandb.log({"loss/training_step": loss.item()}, step=global_step)
            if global_step % 500 == 0:
                wandb.log({
                    "noisy_images": wandb.Image((1 + noisy_images[:8].clamp(-1, 1)) / 2),
                }, step=global_step)

                # Calculate predicted original images for visualization
                reshaped_sqrt_1macum = sqrt_1macum[t].reshape(-1, 1, 1, 1)
                reshaped_sqrt_acum = sqrt_acum[t].reshape(-1, 1, 1, 1)
                predicted_images = (noisy_images - reshaped_sqrt_1macum * predicted_noise) / reshaped_sqrt_acum

                wandb.log({
                    "predicted_images": wandb.Image((1 + predicted_images[:8].clamp(-1, 1)) / 2),
                }, step=global_step)

                with torch.no_grad():
                    samples = sample(model=ema_model, schedule=schedule, step_count=step_count, img_size=images.size()[1:])
                    samples = (samples.clamp(-1, 1) + 1) / 2
                    
                    wandb.log({
                        "generated_images": wandb.Image(samples),
                    }, step=global_step)

            global_step += 1
        
        wandb.log({
            "lr/train_step": scheduler.get_last_lr()[0],
        }, step=global_step)
        scheduler.step()

        epoch_loss = running_loss / len(dataloader)
        wandb.log({"loss/train_epoch": epoch_loss}, step=global_step)
        print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}")

        # Save the model every 5 epochs 
        # if (epoch + 1) % 5 == 0:
        #     torch.save(model.state_dict(), f"model_epoch_{epoch + 1}.pth")
        #     print(f"Model saved at epoch {epoch + 1}")

    # Save the final model  
    wandb.finish()
    print("Training complete.")
    return model

### Sampling
def sample(model: torch.nn.Module, schedule: dict, step_count: int, img_size=(3, 32, 32), num_images=8):
    model.eval()
    betas = schedule['betas']
    alphas = schedule['alphas']
    alphas_cumprod = schedule['alphas_cumprod']
    sqrt_acum = schedule['sqrt_acum']
    sqrt_1macum = schedule['sqrt_1macum']

    sqrt_alphas = torch.sqrt(alphas)
    with torch.no_grad():
        # Start with random noise
        x = torch.randn(num_images, *img_size).to(device)

        for t in range(step_count-1, -1, -1):
            # Compute the predicted noise
            t_tensor = torch.full((num_images,), t, device=device, dtype=torch.long)
            predicted_noise = model(x, t_tensor.unsqueeze(1) / step_count)
            
            # Calculate mean of the previous step
            x_mean = (x - (1 - alphas[t]) * predicted_noise / sqrt_1macum[t]) / sqrt_alphas[t]

            if t > 0:
                # Calculate posterior variance for adding noise
                posterior_var = betas[t] * (1 - alphas_cumprod[t-1])/(1 - alphas_cumprod[t])
                sigma = torch.sqrt(posterior_var + 1e-20)  # Add small epsilon to prevent numerical issues

                x = x_mean + sigma * torch.randn_like(x)

            else:
                x = x_mean
        
        x = x.clamp(-1, 1)
                        
        return x



================================================
File: model.py
================================================
# Standard library imports

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import einsum

from torchvision import datasets, transforms
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader
from tqdm import tqdm

import numpy as np
import matplotlib.pyplot as plt
import math

from unet_components import DoubleConv, Down, Up, OutConv

MODEL_REGISTRY = {}

def register_model(name):
    """
    A decorator to register a model class in the MODEL_REGISTRY.
    """
    def decorator(model_class):
        MODEL_REGISTRY[name] = model_class
        return model_class
    return decorator

def sinusoidal_positional_encoding(timesteps, dim):
    """
    Generates sinusoidal positional encoding.
    Args:
        timesteps: torch.Tensor of shape (batch_size, 1)
        dim: int, the dimension of the encoding.
    Returns:
        torch.Tensor of shape (batch_size, dim)
    """
    half_dim = dim // 2
    freq_embeddings = math.log(10000) / (half_dim - 1)
    freq_embeddings = torch.exp(torch.arange(half_dim, device=timesteps.device) * -freq_embeddings)

    time_embeddings = timesteps.unsqueeze(1) * freq_embeddings.unsqueeze(0)

    final_embeddings = torch.cat((time_embeddings.sin(), time_embeddings.cos()), dim=-1)
    
    if dim % 2 == 1:
        if final_embeddings.shape[1] < dim:
            final_embeddings = F.pad(final_embeddings, (0, 1))
    
    return final_embeddings

@register_model("unet32")
class UNet32(nn.Module):
    def __init__(self, in_channels=3, out_channels=3, latent_dim=256, time_emb_dim=256):
        super(UNet32, self).__init__()
        self.dim3 = latent_dim
        self.dim2 = latent_dim // 2
        self.dim1 = latent_dim // 4

        self.time_mlp = nn.Sequential(
            nn.Linear(time_emb_dim, time_emb_dim * 4),
            nn.SiLU(),
            nn.Linear(time_emb_dim * 4, time_emb_dim)
        )
        self.time_emb_dim = time_emb_dim

        # We expect the input to be of shape (batch_size, 3, 32, 32)

        # Encoding blocks

        self.t_proj1 = nn.Linear(time_emb_dim, self.dim1)
        self.block1 = self._make_block(in_channels, self.dim1)

        self.t_proj2 = nn.Linear(time_emb_dim, self.dim2)
        self.block2 = self._make_block(self.dim1, self.dim2)

        self.t_proj3 = nn.Linear(time_emb_dim, self.dim3)
        self.block3 = self._make_block(self.dim2, self.dim3)

        self.maxpool = nn.MaxPool2d(2)

        # Decoding blocks
        self.upconv1 = nn.ConvTranspose2d(self.dim3, self.dim2, kernel_size=2, stride=2)
        self.t_proj4 = nn.Linear(time_emb_dim, self.dim2)
        self.block4 = self._make_block(self.dim2 * 2, self.dim2)

        self.upconv2 = nn.ConvTranspose2d(self.dim2, self.dim1, kernel_size=2, stride=2)
        self.t_proj5 = nn.Linear(time_emb_dim, self.dim1)
        self.block5 = self._make_block(self.dim1 * 2, self.dim1)
        
        self.t_proj6 = nn.Linear(time_emb_dim, self.dim1)
        self.block6 = nn.Sequential(
            nn.Conv2d(self.dim1, self.dim1, kernel_size=3, padding=1),
            nn.GroupNorm(8, self.dim1),
            nn.SiLU(inplace=True),
            nn.Conv2d(self.dim1, out_channels, kernel_size=1)
        )

        self.init_weights()




    def _make_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.GroupNorm(8, out_channels), # GroupNorm is common in DDPMs
            nn.SiLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.GroupNorm(8, out_channels),
            nn.SiLU(inplace=True)
        )


    def forward(self, x, t):
        t_input = t.float().squeeze(-1)
        if t_input.ndim == 0:
            t_input = t_input.unsqueeze(0)
        sin_emb = sinusoidal_positional_encoding(t_input, self.time_emb_dim)
        temb = self.time_mlp(sin_emb)

        # Encoding
        enc1 = self.block1(x)
        enc1 = enc1 + self.t_proj1(temb)[:, :, None, None]

        enc2 = self.block2(self.maxpool(enc1))
        enc2 = enc2 + self.t_proj2(temb)[:, :, None, None]

        enc3 = self.block3(self.maxpool(enc2))
        enc3 = enc3 + self.t_proj3(temb)[:, :, None, None]

        # Decoding
        dec1 = self.upconv1(enc3)
        dec1 = torch.cat((dec1, enc2), dim=1)
        dec1 = self.block4(dec1)
        dec1 = dec1 + self.t_proj4(temb)[:, :, None, None]

        dec2 = self.upconv2(dec1)
        dec2 = torch.cat((dec2, enc1), dim=1)
        dec2 = self.block5(dec2)
        dec2 = dec2 + self.t_proj5(temb)[:, :, None, None]

        out = dec2 + self.t_proj6(temb)[:, :, None, None]
        out = self.block6(out)

        return out
    
    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.kaiming_uniform_(m.weight, a=0.01, mode='fan_in', nonlinearity='leaky_relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.GroupNorm):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

@register_model("wnet32")
class WNet32(nn.Module):
    def __init__(self, in_channels=3, out_channels=3, latent_dim=256, time_emb_dim=256):
        super(WNet32, self).__init__()
        self.first = UNet32(in_channels, out_channels, latent_dim, time_emb_dim)
        self.second = UNet32(in_channels, out_channels, latent_dim, time_emb_dim)

    def forward(self, x, t):
        x = self.first(x, t)
        x = self.second(x, t)
        return x

@register_model("recurrent_wnet32")
class RecurrentWNet32(nn.Module):
    def __init__(self, in_channels=3, out_channels=3, latent_dim=256, time_emb_dim=256):
        super(RecurrentWNet32, self).__init__()
        self.first = UNet32(in_channels, out_channels, latent_dim, time_emb_dim)
        self.second = UNet32(in_channels, out_channels, latent_dim, time_emb_dim)

    def forward(self, x, t):
        first_x = self.first(x, t)
        second_x = self.second(x, t)
        third_x = self.second(first_x, t)
        return (first_x + second_x + third_x) / 3

@register_model("unet_attn")
class UNetAttn(nn.Module):
    def __init__(self, in_channels=3, out_channels=3, latent_dim=256, time_emb_dim=256):
        super(UNetAttn, self).__init__()
        self.unet = UNet32(in_channels, 16, latent_dim, time_emb_dim)
        self.attn = nn.MultiheadAttention(embed_dim=16, num_heads=8, batch_first=True)
        self.pos_emb = nn.Parameter(torch.randn(32 * 32, 16))
        self.output_layer = nn.Conv2d(16, out_channels, kernel_size=1)

    def forward(self, x, t):
        x = self.unet(x, t)
        b, c, h, w = x.size()

        x_flat = x.view(b, c, -1).permute(0, 2, 1)  # (batch_size, h*w, channels)
        x_flat += self.pos_emb.unsqueeze(0)  # Add positional encoding
        attn_output, _ = self.attn(x_flat, x_flat, x_flat)
        out = attn_output.permute(0, 2, 1).view(b, c, h, w)  # (batch_size, channels, h*w)
        out = self.output_layer(out)
        return out
    
@register_model("unet")
class UNet(nn.Module):
    def __init__(self, in_channels, out_channels, latent_dim, time_emb_dim):
        super(UNet, self).__init__()

        """
        UNet architecture for diffusion models.

        Takes input images of size 32x32 and downsamples them through several blocks:
        32x32 -> 16x16 -> 8x8 -> 4x4, then upsamples back to 32x32.
        """

        self.dim4 = latent_dim
        self.dim3 = latent_dim // 2
        self.dim2 = latent_dim // 4
        self.dim1 = latent_dim // 8

        
        self.time_mlp = nn.Sequential(
            nn.Linear(time_emb_dim, time_emb_dim * 4),
            nn.SiLU(),
            nn.Linear(time_emb_dim * 4, time_emb_dim)
        )

        self.time_emb_dim = time_emb_dim

        # Time projection layers
        self.t_proj1 = nn.Linear(time_emb_dim, 64)
        self.t_proj2 = nn.Linear(time_emb_dim, self.dim1)
        self.t_proj3 = nn.Linear(time_emb_dim, self.dim2)
        self.t_proj4 = nn.Linear(time_emb_dim, self.dim3)
        self.t_proj5 = nn.Linear(time_emb_dim, self.dim4)

        # Encoding blocks
        self.inc = DoubleConv(in_channels, 64)
        self.down1 = Down(64, self.dim1)
        self.down2 = Down(self.dim1, self.dim2)
        self.down3 = Down(self.dim2, self.dim3)
        self.down4 = Down(self.dim3, self.dim4)

        # Decoding blocks
        self.up1 = Up(self.dim4, self.dim3, self.dim3)
        self.up2 = Up(self.dim3, self.dim2, self.dim2)
        self.up3 = Up(self.dim2, self.dim1, self.dim1)
        self.up4 = Up(self.dim1, 64, 64)
        self.outc = OutConv(64, out_channels)

    def forward(self, x, t):
        t_input = t.float().squeeze(-1)
        if t_input.ndim == 0:
            t_input = t_input.unsqueeze(0)
        sin_emb = sinusoidal_positional_encoding(t_input, self.time_emb_dim)
        temb = self.time_mlp(sin_emb)

        x1 = self.inc(x)
        x1 = x1 + self.t_proj1(temb)[:, :, None, None]
        x2 = self.down1(x1)
        x2 = x2 + self.t_proj2(temb)[:, :, None, None]
        x3 = self.down2(x2)
        x3 = x3 + self.t_proj3(temb)[:, :, None, None]
        x4 = self.down3(x3)
        x4 = x4 + self.t_proj4(temb)[:, :, None, None]
        x5 = self.down4(x4)
        x5 = x5 + self.t_proj5(temb)[:, :, None, None]
        x = self.up1(x5, x4)
        x = self.up2(x, x3)
        x = self.up3(x, x2)
        x = self.up4(x, x1)
        logits = self.outc(x)
        return logits


================================================
File: train.py
================================================
import yaml, argparse
from pathlib import Path
import datetime
import torch
import torch.nn as nn
import numpy as np
from diffusion import make_diffusion_schedule, train

def load_yaml(path: str):
    with open(path, "r") as f:
        return yaml.safe_load(f)
    

# Argparse for command line arguments
parser = argparse.ArgumentParser()
parser.add_argument("--config", type=str, help="Path to the config file", default="config/cifar10.yaml")
parser.add_argument("--exp_name", type=str, help="Experiment name (will be auto-generated if not provided)", default=None)
parser.add_argument("--exp_type", type=str, help="Experiment type (e.g., baseline, ablation, tuning)", default="baseline")
parser.add_argument("--epochs", type=int, help="Number of epochs", default=200)
parser.add_argument("--batch_size", type=int, help="Batch size", default=128)
parser.add_argument("--lr", type=float, help="Learning rate", default=2e-4)
parser.add_argument("--latent_dim", type=int, help="Latent dimension", default=512)
parser.add_argument("--step_count", type=int, help="Number of steps", default=1000)
parser.add_argument("--model", type=str, help="Model name", default="unet32")
parser.add_argument("--device", type=str, help="Device to use", default="cuda" if torch.cuda.is_available() else "cpu")
parser.add_argument("--seed", type=int, help="Random seed", default=42)
parser.add_argument("--size", type=int, help="Image size", default=32)
parser.add_argument("--notes", type=str, help="Additional notes about this experiment", default="")

args = parser.parse_args()
config = load_yaml(args.config)

# Generate experiment name if not provided
if args.exp_name is None:
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    lr_str = f"lr{args.lr:.0e}".replace("-", "")  # Convert 5e-5 to lr5e5
    args.exp_name = f"{args.model}_{config['dataset']}_bs{args.batch_size}_{lr_str}_{args.exp_type}_{timestamp}"

config["exp_name"] = args.exp_name
config["latent_dim"] = args.latent_dim
config["step_count"] = args.step_count
config["epochs"] = args.epochs
config["batch_size"] = args.batch_size
config["lr"] = args.lr

if args.device.isdigit():
    torch.cuda.set_device(int(args.device))
    device = torch.device("cuda")
elif args.device == "cuda" and torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

# Initialize random seed
torch.manual_seed(args.seed)
np.random.seed(args.seed)

# Model and optimizer
from model import *
model_cls = MODEL_REGISTRY[args.model]
model = model_cls(
    in_channels=config["channels"],
    out_channels=config["channels"],
    latent_dim=config["latent_dim"],
    time_emb_dim=config["time_emb_dim"]
).to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=config["lr"], weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config["epochs"], eta_min=1e-6)
criterion = nn.MSELoss()

# Diffusion schedule
schedule = make_diffusion_schedule(config["step_count"], device=device)


# Dataset
from data import *
dataset_cls = DATALOADER_REGISTRY[config["dataset"]]
train_loader = dataset_cls(
    batch_size=config["batch_size"],
    resize=(args.size, args.size),
)

# Training loop
import wandb
wandb.init(
    project=config["project"],
    name=config["exp_name"],
    notes=args.notes,
    tags=[config["dataset"], args.model, args.exp_type],
    config={
        "learning_rate": config["lr"],
        "batch_size": config["batch_size"],
        "epochs": config["epochs"],
        "step_count": config["step_count"],
        "beta_schedule": config["beta_schedule"],
        "optimizer": "AdamW",
        "criterion": "MSELoss",
        "weight_decay": 1e-2,
        "scheduler": "CosineAnnealingLR",
        "T_max": config["epochs"],
        "eta_min": 1e-6,
        "clip_grad_norm": 1.0,
        "dataset": config["dataset"],
        "model": args.model,
        "latent_dim": config["latent_dim"],
        "image_size": args.size,
        "seed": args.seed,
        "exp_type": args.exp_type,
        "device": device,
    }
)

if __name__ == "__main__":
    model = train(
        model=model,
        dataloader=train_loader,
        optimizer=optimizer,
        criterion=criterion,
        scheduler=scheduler,
        num_epochs=config["epochs"],
        step_count=config["step_count"],
        schedule=schedule
    )
    


================================================
File: unet_components.py
================================================
import torch
import torch.nn as nn
import torch.nn.functional as F

class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels, mid_channels=None):
        super().__init__()
        if not mid_channels:
            mid_channels = out_channels
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),
            nn.GroupNorm(32, mid_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.GroupNorm(32, out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.double_conv(x)
    
class Down(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.maxpool_conv = nn.Sequential(
            nn.MaxPool2d(2),
            DoubleConv(in_channels, out_channels)
        )

    def forward(self, x):
        return self.maxpool_conv(x)
    
class Up(nn.Module):
    def __init__(self, in_channels, skip_channels, out_channels, bilinear=False):
        super().__init__()
        if bilinear:
            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
            up_out = in_channels
        else:
            self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)
            up_out = out_channels

        self.conv = DoubleConv(skip_channels + up_out, out_channels)
        

    def forward(self, x_low, x_skip):
        x_low = self.up(x_low)
        x = torch.cat([x_skip, x_low], dim=1)

        return self.conv(x)
    
class OutConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)

    def forward(self, x):
        return self.conv(x)
    



